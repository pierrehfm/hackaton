{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier de pollution...\n",
      "\n",
      "=== Nettoyage de base ===\n",
      "\n",
      "=== Nettoyage des chaînes de caractères ===\n",
      "\n",
      "=== Renommage des colonnes ===\n",
      "\n",
      "=== Informations sur le dataset nettoyé ===\n",
      "Nombre de lignes : 630\n",
      "Nombre de colonnes : 15\n",
      "\n",
      "Colonnes :\n",
      "['id', 'gid', 'code_zone', 'zone', 'date', 'date_dif', 'code_pol', 'polluant', 'etat', 'niveau', 'com_court', 'commentaire', 'OBJECTID', 'SHAPE__Area', 'SHAPE__Length']\n",
      "\n",
      "Aperçu des données :\n",
      "   id  gid  code_zone             zone                      date  \\\n",
      "0   1    1       2016  Zone Alpine Ain 2024-01-14 00:00:00+00:00   \n",
      "1   2    2       2016  Zone Alpine Ain 2024-01-13 00:00:00+00:00   \n",
      "2   3    3       2016  Zone Alpine Ain 2024-01-14 00:00:00+00:00   \n",
      "3   4    4       2016  Zone Alpine Ain 2024-01-13 00:00:00+00:00   \n",
      "4   5    5       2016  Zone Alpine Ain 2024-01-14 00:00:00+00:00   \n",
      "\n",
      "                 date_dif  code_pol         polluant                etat  \\\n",
      "0  2024/01/13 00:00:00+00         8  Dioxyde d'azote  PAS DE DEPASSEMENT   \n",
      "1  2024/01/13 00:00:00+00         8  Dioxyde d'azote  PAS DE DEPASSEMENT   \n",
      "2  2024/01/13 00:00:00+00         7            Ozone  PAS DE DEPASSEMENT   \n",
      "3  2024/01/13 00:00:00+00         7            Ozone  PAS DE DEPASSEMENT   \n",
      "4  2024/01/13 00:00:00+00         5  Particules PM10  PAS DE DEPASSEMENT   \n",
      "\n",
      "    niveau com_court commentaire  OBJECTID   SHAPE__Area  SHAPE__Length  \n",
      "0  #19FF19     aucun       aucun         1  3.359408e+09  346225.361125  \n",
      "1  #19FF19     aucun       aucun         2  3.359408e+09  346225.361125  \n",
      "2  #19FF19     aucun       aucun         3  3.359408e+09  346225.361125  \n",
      "3  #19FF19     aucun       aucun         4  3.359408e+09  346225.361125  \n",
      "4  #19FF19     aucun       aucun         5  3.359408e+09  346225.361125  \n",
      "\n",
      "Fichier nettoyé sauvegardé sous : data/output/episodes_de_pollution_nettoyes.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier\n",
    "print(\"Chargement du fichier de pollution...\")\n",
    "df_pollution = pd.read_csv(\"data/episodes_de_pollution_prevus_ou_constates.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "\n",
    "# Nettoyage de base\n",
    "print(\"\\n=== Nettoyage de base ===\")\n",
    "df_pollution.columns = df_pollution.columns.str.strip()  # Nettoyer noms de colonnes\n",
    "df_pollution = df_pollution.drop_duplicates()  # Retirer les doublons\n",
    "df_pollution['date_prevision'] = pd.to_datetime(df_pollution['date_prevision'], errors='coerce')  # Convertir dates\n",
    "df_pollution = df_pollution.dropna(subset=[\"date_prevision\", \"lib_zone\", \"lib_pol\"])  # Supprimer les lignes incomplètes\n",
    "\n",
    "# Nettoyer les chaînes de caractères\n",
    "print(\"\\n=== Nettoyage des chaînes de caractères ===\")\n",
    "for col in ['lib_zone', 'lib_pol', 'etat', 'com_court', 'com_long']:\n",
    "    if col in df_pollution.columns:\n",
    "        df_pollution[col] = df_pollution[col].astype(str).str.strip()\n",
    "\n",
    "# Renommage clair\n",
    "print(\"\\n=== Renommage des colonnes ===\")\n",
    "df_pollution = df_pollution.rename(columns={\n",
    "    \"lib_zone\": \"zone\",\n",
    "    \"date_prevision\": \"date\",\n",
    "    \"lib_pol\": \"polluant\",\n",
    "    \"etat\": \"etat\",\n",
    "    \"couleur\": \"niveau\",\n",
    "    \"com_long\": \"commentaire\"\n",
    "})\n",
    "\n",
    "# Affichage des informations\n",
    "print(\"\\n=== Informations sur le dataset nettoyé ===\")\n",
    "print(f\"Nombre de lignes : {len(df_pollution)}\")\n",
    "print(f\"Nombre de colonnes : {len(df_pollution.columns)}\")\n",
    "print(\"\\nColonnes :\")\n",
    "print(df_pollution.columns.tolist())\n",
    "print(\"\\nAperçu des données :\")\n",
    "print(df_pollution.head())\n",
    "\n",
    "# Sauvegarde du fichier nettoyé\n",
    "output_file = \"data/output/episodes_de_pollution_nettoyes.csv\"\n",
    "df_pollution.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nFichier nettoyé sauvegardé sous : {output_file}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  code_insee_commune   axe     x_deb      y_deb      x_fin      y_fin  \\\n",
      "0        237676517.0   A28  569981.6  6938140.0  568217.60  6935783.5   \n",
      "2        733131555.0  A620  572626.2  6284049.5  572552.90  6283075.0   \n",
      "4        237676517.0   A28  569985.7  6938129.5  568226.06  6935779.0   \n",
      "6        723333075.0  A630  414987.7  6427724.5  414042.70  6427039.0   \n",
      "9        723333075.0  A630  414969.9  6427708.0  414051.80  6427035.0   \n",
      "\n",
      "   longueur code_traficolor  \n",
      "0      3035            RO76  \n",
      "2       834            TO31  \n",
      "4      3042            RO76  \n",
      "6      1030            BX33  \n",
      "9      1000            BX33  \n"
     ]
    }
   ],
   "source": [
    "df_refdir = pd.read_csv(\"data/refDir.csv\", sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "# Nettoyage\n",
    "df_refdir.columns = df_refdir.columns.str.strip()\n",
    "df_refdir = df_refdir.drop_duplicates()\n",
    "\n",
    "# Gérer types de données\n",
    "df_refdir[\"code_insee_commune\"] = df_refdir[\"code_insee_commune\"].astype(str)\n",
    "df_refdir = df_refdir.dropna(subset=[\"x_deb\", \"y_deb\", \"x_fin\", \"y_fin\", \"axe\"])\n",
    "\n",
    "# Filtrer les lignes avec longueurs nulles ou négatives\n",
    "df_refdir = df_refdir[df_refdir[\"longueur\"] > 0]\n",
    "\n",
    "# Colonnes utiles seulement\n",
    "colonnes_utiles = [\n",
    "    \"code_insee_commune\", \"axe\", \"x_deb\", \"y_deb\", \"x_fin\", \"y_fin\", \"longueur\", \"code_traficolor\"\n",
    "]\n",
    "df_refdir = df_refdir[colonnes_utiles]\n",
    "\n",
    "print(df_refdir.head())\n",
    "\n",
    "# Sauvegarde du fichier nettoyé\n",
    "output_file = \"data/output/infra_route_refdir.csv\"\n",
    "df_refdir.to_csv(output_file, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture du fichier DBF en cours...\n",
      "Conversion en CSV en cours...\n",
      "Conversion terminée ! Le fichier a été sauvegardé sous : data/pvo_patrimoine_voirie_pvocomptagecriter.csv\n",
      "Nombre de lignes converties : 2776\n",
      "\n",
      "Aperçu des colonnes :\n",
      "['positionne', 'distanceli', 'nom', 'typecapteu', 'typepostem', 'nbvoies', 'moyennejou', 'debithorai', 'horairedeb', 'identifian', 'identifia0', 'anneerefer', 'estvalide', 'gid']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dbfread import DBF\n",
    "import os\n",
    "\n",
    "# Chemin du fichier DBF\n",
    "dbf_file = 'data/pvo_patrimoine_voirie_pvocomptagecriter.dbf'\n",
    "# Chemin du fichier CSV de sortie\n",
    "csv_file = 'data/pvo_patrimoine_voirie_pvocomptagecriter.csv'\n",
    "\n",
    "# Lire le fichier DBF\n",
    "print(\"Lecture du fichier DBF en cours...\")\n",
    "table = DBF(dbf_file, encoding='latin1')\n",
    "\n",
    "# Convertir en DataFrame pandas\n",
    "df = pd.DataFrame(iter(table))\n",
    "\n",
    "# Sauvegarder en CSV\n",
    "print(\"Conversion en CSV en cours...\")\n",
    "df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Conversion terminée ! Le fichier a été sauvegardé sous : {csv_file}\")\n",
    "print(f\"Nombre de lignes converties : {len(df)}\")\n",
    "print(\"\\nAperçu des colonnes :\")\n",
    "print(df.columns.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture du fichier CSV...\n",
      "\n",
      "=== Informations générales sur le dataset ===\n",
      "Nombre total de lignes : 2776\n",
      "Nombre total de colonnes : 14\n",
      "\n",
      "Types de données par colonne :\n",
      "positionne     object\n",
      "distanceli      int64\n",
      "nom            object\n",
      "typecapteu     object\n",
      "typepostem     object\n",
      "nbvoies         int64\n",
      "moyennejou    float64\n",
      "debithorai    float64\n",
      "horairedeb     object\n",
      "identifian      int64\n",
      "identifia0      int64\n",
      "anneerefer    float64\n",
      "estvalide     float64\n",
      "gid             int64\n",
      "dtype: object\n",
      "\n",
      "=== Analyse des valeurs manquantes ===\n",
      "            Valeurs manquantes  Pourcentage\n",
      "moyennejou                 139     5.007205\n",
      "debithorai                 139     5.007205\n",
      "horairedeb                 139     5.007205\n",
      "anneerefer                 139     5.007205\n",
      "estvalide                 2776   100.000000\n",
      "\n",
      "=== Statistiques descriptives des variables numériques ===\n",
      "        distanceli      nbvoies     moyennejou    debithorai   anneerefer\n",
      "count  2776.000000  2776.000000    2637.000000   2637.000000  2637.000000\n",
      "mean     89.569524     1.636527   13691.774365   1251.195677  2022.838074\n",
      "std     165.868604     0.761756   15991.983171   1460.282670     0.589442\n",
      "min       0.000000     1.000000      10.000000      0.000000  2019.000000\n",
      "25%       0.000000     1.000000    3914.000000    411.000000  2023.000000\n",
      "50%      79.000000     1.000000    7118.000000    678.000000  2023.000000\n",
      "75%     102.000000     2.000000   15230.000000   1412.000000  2023.000000\n",
      "max     999.000000     4.000000  103574.000000  17036.000000  2023.000000\n",
      "\n",
      "=== Analyse des valeurs uniques pour les variables catégorielles ===\n",
      "\n",
      "Valeurs uniques dans typecapteu:\n",
      "typecapteu\n",
      "Capteur inductif Criter    2776\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valeurs uniques dans typepostem:\n",
      "typepostem\n",
      "Comptage tous véhicules    2566\n",
      "Comptage vélo               210\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valeurs uniques dans horairedeb:\n",
      "horairedeb\n",
      "08h00    719\n",
      "17h00    634\n",
      "18h00    384\n",
      "07h00    354\n",
      "16h00    195\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Nettoyage des données ===\n",
      "\n",
      "=== Création des visualisations ===\n",
      "\n",
      "Dataset nettoyé sauvegardé sous : data/output/pvo_patrimoine_voirie_pvocomptagecriter_nettoye.csv\n",
      "\n",
      "Visualisations sauvegardées sous :\n",
      "- data/distributions_numeriques.png\n",
      "- data/boxplots_numeriques.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration de l'affichage pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 150)\n",
    "\n",
    "# Lecture du fichier CSV\n",
    "print(\"Lecture du fichier CSV...\")\n",
    "df = pd.read_csv('data/pvo_patrimoine_voirie_pvocomptagecriter.csv')\n",
    "\n",
    "# Affichage des informations générales\n",
    "print(\"\\n=== Informations générales sur le dataset ===\")\n",
    "print(f\"Nombre total de lignes : {len(df)}\")\n",
    "print(f\"Nombre total de colonnes : {len(df.columns)}\")\n",
    "print(\"\\nTypes de données par colonne :\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Analyse des valeurs manquantes\n",
    "print(\"\\n=== Analyse des valeurs manquantes ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_info = pd.DataFrame({\n",
    "    'Valeurs manquantes': missing_values,\n",
    "    'Pourcentage': missing_percentage\n",
    "})\n",
    "print(missing_info[missing_info['Valeurs manquantes'] > 0])\n",
    "\n",
    "# Statistiques descriptives pour les colonnes numériques\n",
    "numeric_columns = ['distanceli', 'nbvoies', 'moyennejou', 'debithorai', 'anneerefer']\n",
    "print(\"\\n=== Statistiques descriptives des variables numériques ===\")\n",
    "print(df[numeric_columns].describe())\n",
    "\n",
    "# Analyse des valeurs uniques pour les colonnes catégorielles\n",
    "categorical_columns = ['typecapteu', 'typepostem', 'horairedeb']\n",
    "print(\"\\n=== Analyse des valeurs uniques pour les variables catégorielles ===\")\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\nValeurs uniques dans {col}:\")\n",
    "    print(df[col].value_counts().head())\n",
    "\n",
    "# Nettoyage des données\n",
    "print(\"\\n=== Nettoyage des données ===\")\n",
    "\n",
    "# Conversion et nettoyage des colonnes numériques\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Nettoyage des colonnes catégorielles\n",
    "for col in categorical_columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Création de visualisations\n",
    "print(\"\\n=== Création des visualisations ===\")\n",
    "\n",
    "# 1. Distribution des variables numériques\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, col in enumerate(numeric_columns, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(data=df, x=col, bins=30)\n",
    "    plt.title(f'Distribution de {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/output/distributions_numeriques.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Boîtes à moustaches pour les variables numériques\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[numeric_columns])\n",
    "plt.title('Distribution des variables numériques (boîtes à moustaches)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/output/boxplots_numeriques.png')\n",
    "plt.close()\n",
    "\n",
    "# Sauvegarde du dataset nettoyé\n",
    "df.to_csv('data/output/pvo_patrimoine_voirie_pvocomptagecriter_nettoye.csv', index=False)\n",
    "print(\"\\nDataset nettoyé sauvegardé sous : data/output/pvo_patrimoine_voirie_pvocomptagecriter_nettoye.csv\")\n",
    "print(\"\\nVisualisations sauvegardées sous :\")\n",
    "print(\"- data/distributions_numeriques.png\")\n",
    "print(\"- data/boxplots_numeriques.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in c:\\python312\\lib\\site-packages (2.9.10)\n",
      "Requirement already satisfied: sqlalchemy in c:\\python312\\lib\\site-packages (2.0.40)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\python312\\lib\\site-packages (from sqlalchemy) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\python312\\lib\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary sqlalchemy python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Structure des données ===\n",
      "Type de données principal: <class 'dict'>\n",
      "Clés principales: dict_keys(['type', 'name', 'crs', 'features'])\n",
      "\n",
      "Nombre de features: 630\n",
      "\n",
      "Structure d'un feature:\n",
      "- Type: Feature\n",
      "\n",
      "Propriétés disponibles:\n",
      "- id: <class 'int'>\n",
      "- gid: <class 'int'>\n",
      "- code_zone: <class 'int'>\n",
      "- lib_zone: <class 'str'>\n",
      "- date_ech: <class 'str'>\n",
      "- date_dif: <class 'str'>\n",
      "- code_pol: <class 'int'>\n",
      "- lib_pol: <class 'str'>\n",
      "- etat: <class 'str'>\n",
      "- couleur: <class 'str'>\n",
      "- com_court: <class 'str'>\n",
      "- com_long: <class 'str'>\n",
      "- OBJECTID: <class 'int'>\n",
      "- SHAPE__Area: <class 'float'>\n",
      "- SHAPE__Length: <class 'float'>\n",
      "\n",
      "Données sauvegardées dans donnees_pollution.csv\n",
      "\n",
      "=== Aperçu des données ===\n",
      "   id  code_zone         lib_zone                  date_ech  \\\n",
      "0   1       2016  Zone Alpine Ain 2024-01-14 00:00:00+00:00   \n",
      "1   2       2016  Zone Alpine Ain 2024-01-13 00:00:00+00:00   \n",
      "2   3       2016  Zone Alpine Ain 2024-01-14 00:00:00+00:00   \n",
      "3   4       2016  Zone Alpine Ain 2024-01-13 00:00:00+00:00   \n",
      "4   5       2016  Zone Alpine Ain 2024-01-14 00:00:00+00:00   \n",
      "\n",
      "                   date_dif  code_pol          lib_pol                etat  \\\n",
      "0 2024-01-13 00:00:00+00:00         8  Dioxyde d'azote  PAS DE DEPASSEMENT   \n",
      "1 2024-01-13 00:00:00+00:00         8  Dioxyde d'azote  PAS DE DEPASSEMENT   \n",
      "2 2024-01-13 00:00:00+00:00         7            Ozone  PAS DE DEPASSEMENT   \n",
      "3 2024-01-13 00:00:00+00:00         7            Ozone  PAS DE DEPASSEMENT   \n",
      "4 2024-01-13 00:00:00+00:00         5  Particules PM10  PAS DE DEPASSEMENT   \n",
      "\n",
      "   couleur com_court com_long longitude   latitude  \n",
      "0  #19FF19     aucun    aucun  5.511349  46.264469  \n",
      "1  #19FF19     aucun    aucun  5.511349  46.264469  \n",
      "2  #19FF19     aucun    aucun  5.511349  46.264469  \n",
      "3  #19FF19     aucun    aucun  5.511349  46.264469  \n",
      "4  #19FF19     aucun    aucun  5.511349  46.264469  \n",
      "\n",
      "=== Informations sur les données ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 630 entries, 0 to 629\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype              \n",
      "---  ------     --------------  -----              \n",
      " 0   id         630 non-null    int64              \n",
      " 1   code_zone  630 non-null    int64              \n",
      " 2   lib_zone   630 non-null    object             \n",
      " 3   date_ech   630 non-null    datetime64[ns, UTC]\n",
      " 4   date_dif   630 non-null    datetime64[ns, UTC]\n",
      " 5   code_pol   630 non-null    int64              \n",
      " 6   lib_pol    630 non-null    object             \n",
      " 7   etat       630 non-null    object             \n",
      " 8   couleur    630 non-null    object             \n",
      " 9   com_court  630 non-null    object             \n",
      " 10  com_long   630 non-null    object             \n",
      " 11  longitude  630 non-null    object             \n",
      " 12  latitude   630 non-null    object             \n",
      "dtypes: datetime64[ns, UTC](2), int64(3), object(8)\n",
      "memory usage: 64.1+ KB\n",
      "None\n",
      "\n",
      "=== Script SQL pour création de la table ===\n",
      "\n",
      "CREATE TABLE IF NOT EXISTS pollution_air (\n",
      "    id INTEGER PRIMARY KEY,\n",
      "    code_zone INTEGER,\n",
      "    lib_zone VARCHAR(255),\n",
      "    date_ech TIMESTAMP,\n",
      "    date_dif TIMESTAMP,\n",
      "    code_pol INTEGER,\n",
      "    lib_pol VARCHAR(255),\n",
      "    etat VARCHAR(50),\n",
      "    couleur VARCHAR(7),\n",
      "    com_court TEXT,\n",
      "    com_long TEXT,\n",
      "    longitude FLOAT,\n",
      "    latitude FLOAT\n",
      ");\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# URL de l'API\n",
    "API_URL = \"https://www.data.gouv.fr/fr/datasets/r/66c8dc0e-c4b5-472e-96ea-f957edfba50f\"\n",
    "\n",
    "def recuperer_donnees():\n",
    "    \"\"\"Récupère les données brutes de l'API\"\"\"\n",
    "    try:\n",
    "        response = requests.get(API_URL)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erreur lors de la récupération des données: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyser_structure(data):\n",
    "    \"\"\"Analyse la structure des données\"\"\"\n",
    "    print(\"\\n=== Structure des données ===\")\n",
    "    print(f\"Type de données principal: {type(data)}\")\n",
    "    print(f\"Clés principales: {data.keys()}\")\n",
    "    \n",
    "    if 'features' in data:\n",
    "        print(f\"\\nNombre de features: {len(data['features'])}\")\n",
    "        if len(data['features']) > 0:\n",
    "            premier_feature = data['features'][0]\n",
    "            print(\"\\nStructure d'un feature:\")\n",
    "            print(f\"- Type: {premier_feature.get('type')}\")\n",
    "            print(\"\\nPropriétés disponibles:\")\n",
    "            for key, value in premier_feature['properties'].items():\n",
    "                print(f\"- {key}: {type(value)}\")\n",
    "\n",
    "def nettoyer_donnees(data):\n",
    "    \"\"\"Nettoie et transforme les données\"\"\"\n",
    "    features = data.get('features', [])\n",
    "    donnees_nettoyees = []\n",
    "    \n",
    "    for feature in features:\n",
    "        if 'properties' in feature and 'geometry' in feature:\n",
    "            proprietes = feature['properties']\n",
    "            geometrie = feature['geometry']\n",
    "            \n",
    "            # Extraction du premier point des coordonnées du polygone\n",
    "            coords = geometrie['coordinates'][0][0] if geometrie['coordinates'] else [None, None]\n",
    "            \n",
    "            # Création d'un dictionnaire nettoyé\n",
    "            donnee_nettoyee = {\n",
    "                'id': proprietes.get('id'),\n",
    "                'code_zone': proprietes.get('code_zone'),\n",
    "                'lib_zone': proprietes.get('lib_zone'),\n",
    "                'date_ech': pd.to_datetime(proprietes.get('date_ech')),\n",
    "                'date_dif': pd.to_datetime(proprietes.get('date_dif')),\n",
    "                'code_pol': proprietes.get('code_pol'),\n",
    "                'lib_pol': proprietes.get('lib_pol'),\n",
    "                'etat': proprietes.get('etat'),\n",
    "                'couleur': proprietes.get('couleur'),\n",
    "                'com_court': proprietes.get('com_court'),\n",
    "                'com_long': proprietes.get('com_long'),\n",
    "                'longitude': coords[0],\n",
    "                'latitude': coords[1]\n",
    "            }\n",
    "            donnees_nettoyees.append(donnee_nettoyee)\n",
    "    \n",
    "    return pd.DataFrame(donnees_nettoyees)\n",
    "\n",
    "def sauvegarder_csv(df, nom_fichier='donnees_pollution.csv'):\n",
    "    \"\"\"Sauvegarde les données dans un fichier CSV\"\"\"\n",
    "    try:\n",
    "        df.to_csv(nom_fichier, index=False)\n",
    "        print(f\"\\nDonnées sauvegardées dans {nom_fichier}\")\n",
    "        \n",
    "        # Afficher un aperçu des données et des informations\n",
    "        print(\"\\n=== Aperçu des données ===\")\n",
    "        print(df.head())\n",
    "        print(\"\\n=== Informations sur les données ===\")\n",
    "        print(df.info())\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde du fichier CSV: {e}\")\n",
    "\n",
    "def main():\n",
    "    # Récupération des données\n",
    "    donnees_brutes = recuperer_donnees()\n",
    "    \n",
    "    if donnees_brutes:\n",
    "        # Analyse de la structure\n",
    "        analyser_structure(donnees_brutes)\n",
    "        \n",
    "        # Nettoyage des données\n",
    "        df_nettoye = nettoyer_donnees(donnees_brutes)\n",
    "        \n",
    "        # Sauvegarde en CSV\n",
    "        sauvegarder_csv(df_nettoye)\n",
    "        \n",
    "        # Création du script SQL pour la table\n",
    "        print(\"\\n=== Script SQL pour création de la table ===\")\n",
    "        print(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pollution_air (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    code_zone INTEGER,\n",
    "    lib_zone VARCHAR(255),\n",
    "    date_ech TIMESTAMP,\n",
    "    date_dif TIMESTAMP,\n",
    "    code_pol INTEGER,\n",
    "    lib_pol VARCHAR(255),\n",
    "    etat VARCHAR(50),\n",
    "    couleur VARCHAR(7),\n",
    "    com_court TEXT,\n",
    "    com_long TEXT,\n",
    "    longitude FLOAT,\n",
    "    latitude FLOAT\n",
    ");\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(\"Impossible de procéder au traitement des données\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ episodes_de_pollution_nettoyes.csv chargé avec succès dans la table episodes_pollution\n",
      "✅ infra_route_refdir.csv chargé avec succès dans la table infrastructure_route\n",
      "✅ pvo_patrimoine_voirie_pvocomptagecriter_nettoye.csv chargé avec succès dans la table comptage_trafic\n",
      "✅ donnees_pollution.csv chargé avec succès dans la table pollution_air\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration de la connexion PostgreSQL\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_HOST = os.getenv('DB_HOST', 'localhost')\n",
    "DB_PORT = os.getenv('DB_PORT', '5432')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "# Création de l'URL de connexion\n",
    "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Création du moteur SQLAlchemy\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# Fonction pour charger un fichier CSV dans une table PostgreSQL\n",
    "def load_csv_to_postgres(csv_file, table_name):\n",
    "    try:\n",
    "        # Lecture du fichier CSV\n",
    "        df = pd.read_csv(f\"data/output/{csv_file}\")\n",
    "        \n",
    "        # Chargement dans PostgreSQL\n",
    "        df.to_sql(\n",
    "            name=table_name,\n",
    "            con=engine,\n",
    "            if_exists='replace',\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"✅ {csv_file} chargé avec succès dans la table {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du chargement de {csv_file}: {str(e)}\")\n",
    "\n",
    "# Chargement des fichiers\n",
    "files_to_load = {\n",
    "    'episodes_de_pollution_nettoyes.csv': 'episodes_pollution',\n",
    "    'infra_route_refdir.csv': 'infrastructure_route',\n",
    "    'pvo_patrimoine_voirie_pvocomptagecriter_nettoye.csv': 'comptage_trafic',\n",
    "    'donnees_pollution.csv': 'pollution_air'\n",
    "    \n",
    "}\n",
    "\n",
    "for csv_file, table_name in files_to_load.items():\n",
    "    load_csv_to_postgres(csv_file, table_name) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
